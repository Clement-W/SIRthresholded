---
title: "How to use the SIRthresholded package"
author: "ClÃ©ment Weinreich"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
    %\VignetteIndexEntry{How to use the SIRthresholded package}
    %\VignetteEngine{knitr::rmarkdown}
    %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo=TRUE,eval=TRUE,comment="#>")
```

This package offers an implementation of the $SIR$ (Sliced Inverse Regression) method, along with a thresholded version of $SIR$ that allows variable selection. The purpose of this vignettes is to explain the function implemented in the package, and how they are used. The first section is dedicated to the theoretical explaination of $SIR$, and its thresholded version. Reading this section is therefore optional, but can be useful to better understand how the package functions work.

## How SIR and SIR thresholded works

### The semi-parametric model

Consider the following semi-parametric model:
$$Y = g(\beta'X,\epsilon)$$
where:

- $Y \in \mathbb{R}$ is a univariate response variable
- $X \in \mathbb{R}^p$ is a $p$-dimensional covariates such as $\mathbb{E}(X)=\mu$ and $\mathbb{V}(X)=\Sigma$
- $\beta$ is a vector generating a direction of $\mathbb{R}^p$
- $\epsilon$ is the random error term, assumed independant of $X$
- $g$ is an arbitrary link function (unknown)

It is common to present the model as $Y\perp X\mid \beta'X$ which means that $Y$ is independent of $X$ conditional on $\beta'X$. Thus, it is possible to replace $X\in\mathbb{R}^p$ with the index $\beta'X\in\mathbb{R}$ without loss of information in the regression of $Y$ on $X$. Thus, we do dimension reduction of the explanatory part $X$ from $p$-dimension to $1$-dimension, without specifying either the relationship between the index $\beta'X$ and $Y$ or the distribution of the error term $\epsilon$. 

In the framework of this model, $g$ being unknown, we cannot identify $\beta$, but we can identify its direction. We then define the $EDR$ (Effective Dimension Reduction) space $E$ as the linear subspace of $\mathbb{R}^p$ generated by $\beta$. Any vector belonging to $E$ is then an $EDR$ direction.

### The SIR method

The $SIR$ method is used to obtain the estimate of an $EDR$ direction. In theory, the $SIR$ method requires a condition on the distribution of the explanatory variable $X$ called the linearity condition: $\forall b \in\mathbb{R}^p, \mathbb{E}[b'X\mid\beta'X]$ is linear in $X'\beta$. In practice, it is impossible to verify this assumption because $\beta$ is unknown and unidentifiable. However, this assumption is valid when the distribution of $X$ is elliptical, which is the case for the multidimensional normal distribution.

Let's now consider a monotonic transformation $T$ (which in the case of the SIR method will correspond to a slicing function). Under the previous model and the linearity condition, (Duan & Li, 1991) showed that the centered inverse regression curve verifies :
$$\mathbb{E}[X\mid T(Y)] - \mu \in \mbox{Span}(\Sigma\beta)$$
Thus, the subspace generated by this curve, ${\mathbb{E}[X \mid T(Y)] - \mathbb{E}[X] : Y \in \mathcal{Y}}$ (where $\mathcal{Y}$ is the support for the response variable $Y$), belongs to the $EDR$ space. Therefore, the centered inverse regression curve can clearly be used to find the $EDR$ space.

A direct consequence of this result, is that the variance covariance matrix of this curve, $\Gamma = \mathbb{V}(\mathbb{E}[X \mid T(Y)])$, is degenerate in any direction $\Sigma$-orthogonal to $\beta$. Therefore, the eigenvector associated with the largest nonzero eigenvalue of the matrix of interest $\Sigma^{-1}\Gamma$ is an $EDR$ direction, so it generates the $EDR$ space $E$.

In the slicing step of the $SIR$ method, the support of $Y$ is sliced into $H$ slices $\{s_1,\dots,s_H\}$. With a function $T$ corresponding to this slicing, the matrix $\Gamma$ can then be written as 
$$\Gamma=\sum_{h=1}^Hp_h(m_h-\mu)(m_h-\mu)'$$
where $p_h=P(Y\in s_h)$ is the theoretical proportion of $Y_i$ falling in the $s_h$ slice and $m_h=\mathbb{E}[X\mid Y\in s_h]$ is the theoretical mean of $X_i$ associated with $Y_i$ falling in the $s_h$ slice. This matrix $\Gamma$ is therefore easy to estimate.

\

Let us consider a sample $\{(X_i,Y_i), i=1,\dots,n\}$ generated from the previous model. In practice, it is necessary to replace the theoretical moments $\mu, \Sigma, p_h$ and $m_h$ by the empirical moments. Then, an estimator of a basis of the $EDR$ space $E$ is obtained via the eigenvector $\hat{b}$ associated to the largest eigenvalue of the estimator $\widehat{\Sigma}_n^{-1}\widehat{\Gamma}_n$ of $\Sigma^{-1}\Gamma$ where : 

- $\widehat{\Sigma}_n=  \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X}_n)(X_i -\bar{X}_n)'$ is the empirical variance covariance matrix of $X$


- $\widehat{\Gamma}_n= \sum_{h=1}^H\hat p_{h,n} (\hat m_{h,n} - \bar{X}_n)(\hat m_{h,n} -\bar{X}_n)'$ is the empirical variance covariance matrix of the empirical means of $X$ per slices $s_h$

with :

- $\bar{X}_n= \frac{1}{n}\sum_{i=1}^nX_i,$ the empirical mean of the sample

- $\hat p_{h,n} = \frac{1}{n}\sum_{i=1}^{n} \mathbb{I} {[Y_i\in {s_h} ]}  = \frac{\hat{n}_{h,n}}{n}$ the empirical proportion of $Y_i$  falling in the slice $s_h$ (where $n_{h,n}$ is the number of $Y$ falling in the slice $s_h$)

- $\hat m_{h,n}=\frac{1}{\hat{n}_{h,n}  }\sum_{i\in {s_h}}X_i$ the empirical mean of $X_i$ such that $Y_i$ falls into the slice $h$


### Variable selection by soft or hard thresholding

It is possible to perform variable selection in SIR regression. The method presented here is a computational method using soft or hard thresholding of the matrix of interest.
Let us define the soft thresholding ($S$ as in soft) and hard thresholding ($H$ as in hard) operators that apply to any matrix $M$ of dimension $p\times p$. For any real $\lambda\geq 0$ and $(i,j)\in \{1,\dots,p\}\times \{1,\dots,p\}$ :


$$S_\lambda(M)_{i,j}= sign(M_{i,j}) \times \begin{cases}
      \lvert M_{i,j} \rvert -\lambda & \mbox{if } \lvert M_{i,j} \rvert -\lambda >0, \\
      0 & \text{otherwise.}
    \end{cases}$$
    
$$H_\lambda(M)_{i,j}= \begin{cases}
       M_{i,j}  & \mbox{if } \lvert M_{i,j} \rvert -\lambda >0, \\
      0 & \text{otherwise.}
    \end{cases}$$
    
with $sign$ the function that returns the sign of the considered element ($1$ if positive, $0$ if null and $-1$ if negative).

We then introduce two new methods:

- $ST-SIR$ for soft thresholding of the matrix of interest of the SIR approach, which considers the matrix $S_\lambda(\widehat{\Sigma}_n^{-1}\widehat{\Gamma}_n)$,
- $HT-SIR$ for hard thresholding of the matrix of interest of the SIR approach, which considers the matrix $H_\lambda(\widehat{\Sigma}_n^{-1}\widehat{\Gamma}_n)$.

For each approach, an eigenvector associated to the largest eigenvalue of $S_\lambda(\widehat{\Sigma}_n^{-1}\widehat{\Gamma}_n)$ or $H_\lambda(\widehat{\Sigma}_n^{-1}\widehat{\Gamma}_n)$ is computed, for a given value of $\lambda$.

The choice of the thresholding hyper-parameter $\lambda$ must provide a balance between

- correct variable selection (by tilting some of the components of the considered eigenvector to $0$)
- low distortion of the estimated direction $\hat{b}$ 


Then, once the $p^{\star}$ useful variables have been identified, a new $SIR$ model can be estimated on the basis of these $p^{\star}$ selected variables. Thus, the estimation of $\beta$ will be easier because the estimation is done in a space of reduced dimension.


## Overview

### `SIR` function

Apply a single-index $SIR$ on $(X,Y)$ with $H$ slices. This function allows to obtain an estimate of a basis of the $EDR$ (Effective Dimension Reduction) space via the eigenvector $\hat{b}$ associated with the largest nonzero eigenvalue of the matrix of interest $\widehat{\Sigma}_n^{-1}\widehat{\Gamma}_n$. Thus, $\hat{b}$ is an $EDR$ direction.

### `SIR_bootstrap` function

Apply a single-index $SIR$ on $B$ bootstraped samples of $(X,Y)$ with $H$ slices. 

### `SIR_threshold` function

Apply a single-index $SIR$ on $(X,Y)$ with $H$ slices, with a parameter $\lambda$ which threshold the interest matrix $\widehat{\Sigma}_n^{-1}\widehat{\Gamma}_n$.

### `SIR_threshold_opt` function

Apply a single-index SIR on $(X,Y)$ with $H$ slices, with a thresholding of the interest matrix $\widehat{\Sigma}_n^{-1}\widehat{\Gamma}_n$ by an optimal parameter $\lambda_{opt}$. The $\lambda_{opt}$ is found automatically among a vector of `n_lambda` $\lambda$, starting from 0 to the maximum value of $\widehat{\Sigma}_n^{-1}\widehat{\Gamma}_n$. For each feature of $X$, the number of $\lambda$ associated with a selection of this feature is stored (in a vector of size $p$). This vector is sorted in a decreasing way. Then, thanks to `strucchange::breakpoints`, a breakpoint is found in this sorted vector. The coefficients of the variables at the left of the breakpoint, tend to be automatically toggled to 0 due to the thresholding operation based on $\lambda_{opt}$, and so should be removed (useless variables). Finally, $\lambda_{opt}$ corresponds to the first $\lambda$ such that the associated $\hat{\beta}$ provides the same number of zeros as the breakpoint's value.

For example, for $X \in \mathbb{R}^{10}$ and `n_lambda=100`, this sorted vector can look like this :

| X10 | X3 | X8 | X5 | X7 | X9 | X4 | X6 | X2 | X1  |
|-----|----|----|----|----|----|----|----|----|-----|
| 2   | 3  | 3  | 4  | 4  | 4  | 6  | 10 | 95 | 100 |

Here, the breakpoint would be 8.

### `SIR_threshold_bootstrap` function

Apply a single-index optimally thresholded $SIR$ with $H$ slices on `n_replications` bootstraped replications of $(X,Y)$. The optimal number of selected variables is the number of selected variables that came back most often among the replications performed. From this, we can get the corresponding $\hat{\beta}$ and $\lambda_{opt}$ that produce the same number of selected variables in the result of `SIR_threshold_opt`. 

This method allow to validate the robustness of the model.

## Example with simulated data

### Generate the data

Let's create $Y = (X\beta)^3 + \epsilon$ with:

- $n=200$ (Sample size)
- $X\in \mathbb{R}^p$ and $X\sim\mathcal{N}(0,\mathbb{I}_p)$
- $p=30$ and $p^\star=10$ ($10$ relevant variables in $X$)
- $\epsilon\sim\mathcal{N}(0,20)$ and $\epsilon \perp \!\!\! \perp X$.

```{r}
set.seed(1)
n <- 200 # Sample size
p <- 30 # Number of variables in X
p_star <- 10 # Number of relevant variables in X
X <- mvtnorm::rmvnorm(n,sigma=diag(p)) # X ~ N(0,I_p)
dimnames(X) <- list(1:n, paste("X", 1:p, sep = "")) # Rename columns of X

eps <- rnorm(n, sd = 20) # Error

beta <- matrix(c(rep(1,p_star),rep(0,p-p_star)),ncol=1) # Beta = Heaviside function
rownames(beta) <- colnames(X) # Rename rows of X

Y <- (X %*% beta)**3 + eps # The model
```

We can now plot Y versus the reconstructed index $X\beta$
```{r,fig.align='center',fig.width=4,fig.asp=1}
par(mar=c(5,4,1,1)+0.1)
plot(X %*% beta, Y, xlab = "true index")
```

### Create a SIR model

```{r}
library(SIRthresholded)
```

The `SIR` function is applied to these data.
```{r}
res_SIR = SIR(Y = Y, X = X, H = 10,graphic = FALSE)
```

The object `res_SIR` is an object of class `SIR` and contains many numerical results, summarized in the S3 methods `print` and `summary`.
```{r}
summary(res_SIR)
```






## Example with ?????? dataset

```{r, fig.show='hold'}
plot(1:10)
plot(10:1)
```


You can write math expressions, e.g. $Y = X\beta + \epsilon$, footnotes^[A footnote here.], and tables

## References

Duan, N., & Li, K.-C. (1991). Slicing Regression : A Link-Free Regression Method. The Annals of Statistics, 19(2), 505â530.